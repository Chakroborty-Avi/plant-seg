{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PlantSeg introduction","text":"<p>PlantSeg is a tool for 3D and 2D segmentation. The methods used are very generic and can be used for any instance segmentation workflow, but they are tuned towards cell segmentation in plant tissue. The tool is fundamentally composed of two main steps.</p> <p></p> <ul> <li> <p>Cell boundary predictions: Where a convolutional neural network is used to extract a voxel wise boundary classification. The neural network can filter out very different types/intensities of noise, homogenizing the signal strength and fixing imaging defects (such as missing/blurred cell boundaries).</p> </li> <li> <p>Cell Segmentation as graph partitioning: The output of the first step can be used directly for automated segmentation. We implemented four different algorithms for segmentation, each with peculiar features.  This approach is especially well suited for segmenting densely packed cells.</p> </li> </ul> <p>For a complete description of the methods used, please check out our manuscript.</p> <p>If you find PlantSeg useful, please cite:</p> <pre><code>@article{wolny2020accurate,\n  title={Accurate and versatile 3D segmentation of plant tissues at cellular resolution},\n  author={Wolny, Adrian and Cerrone, Lorenzo and Vijayan, Athul and Tofanelli, Rachele and Barro, Amaya Vilches and Louveaux, Marion and Wenzl, Christian and Strauss, S{\\\"o}ren and Wilson-S{\\'a}nchez, David and Lymbouridou, Rena and others},\n  journal={Elife},\n  volume={9},\n  pages={e57613},\n  year={2020},\n  publisher={eLife Sciences Publications Limited}\n}\n</code></pre>"},{"location":"chapters/getting_started/contributing/","title":"Contribute to PlantSeg","text":"<p>PlantSeg is an open-source project and we welcome contributions from the community. There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into PlantSeg itself.</p>"},{"location":"chapters/getting_started/contributing/#getting-started","title":"Getting Started","text":"<p>To set up the development environment, run:</p> <pre><code>mamba env create -f environment-dev.yml\nconda activate plant-seg-dev\n</code></pre> <p>To install PlantSeg in development mode, run:</p> <pre><code>pip install -e . --no-deps\n</code></pre>"},{"location":"chapters/getting_started/contributing/#testing","title":"Testing","text":"<p>In order to run tests make sure that <code>pytest</code> is installed in your conda environment. You can run your tests simply with <code>python -m pytest</code> or <code>pytest</code>. For the latter to work you need to install <code>plantseg</code> locally in \"develop mode\" with <code>pip install -e .</code></p>"},{"location":"chapters/getting_started/installation/","title":"Installation","text":""},{"location":"chapters/getting_started/installation/#prerequisites-for-conda-package","title":"Prerequisites for Conda package","text":"<ul> <li>Linux or Windows</li> <li>(Optional) Nvidia GPU with official Nvidia drivers installed</li> <li>Native MacOS installation (not yet M1) coming soon.</li> </ul>"},{"location":"chapters/getting_started/installation/#install-mamba","title":"Install Mamba","text":"<p>Fist step is to install <code>mamba</code>, which is a faster alternative to <code>conda</code>. If you have Anaconda/Miniconda installed, you can install Mamba in your base environment. Otherwise we suggest to use Miniconda, because it is lighter than Anaconda and install fewer unnecessary packages. Check the Mamba documentation for more details.</p> LinuxWindows <p>To download Miniconda open a terminal and type:</p> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Then install by typing:</p> <pre><code>bash ./Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>and follow the installation instructions. The <code>Miniconda3-latest-Linux-x86_64.sh</code> file can be deleted now.</p> <p>Miniconda can be downloaded from miniconda. Download the executable <code>.exe</code> for your Windows version and follow the installation instructions.</p> <p>When the Miniconda installation is complete, run:</p> <pre><code>conda install -c conda-forge mamba\n</code></pre>"},{"location":"chapters/getting_started/installation/#install-plantseg-using-mamba","title":"Install PlantSeg using Mamba","text":"<ul> <li> <p>GPU version, CUDA=12.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=12.1 pyqt plant-seg\n</code></pre> </li> <li> <p>GPU version, CUDA=11.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=11.8 pyqt plant-seg\n</code></pre> </li> <li> <p>CPU version</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch cpuonly pyqt plant-seg\n</code></pre> </li> </ul> <p>The above command will create new conda environment <code>plant-seg</code> together with all required dependencies.</p>"},{"location":"chapters/getting_started/installation/#install-newer-versions","title":"Install Newer Versions","text":"<p>If you want to install a specific version of PlantSeg that is not available on <code>conda-forge</code>, you can install it from the <code>lcerrone</code> channel. For example, you can run the following command:</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge -c lcerrone pytorch pytorch-cuda=12.1 pyqt plantseg\n</code></pre> <p>Difference between <code>conda-forge</code> and <code>lcerrone</code> channels:</p> <ul> <li> <p>conda-forge/plant-seg:  </p> </li> <li> <p>lcerrone/plantseg:  </p> </li> </ul> <p>Ultimately you may download this repo and install it from source for the latest version.</p>"},{"location":"chapters/getting_started/installation/#optional-dependencies","title":"Optional dependencies","text":"<p>(not fully tested on Windows)</p> <p>Some types of compressed tiff files require an additional package to be load correctly (e.g.: Zlib, ZSTD, LZMA, ...). To run PlantSeg on those stacks, you need to install <code>imagecodecs</code>. In the terminal:</p> <pre><code>conda activate plant-seg\npip install imagecodecs\n</code></pre> <p>Experimental support for SimpleITK watershed segmentation has been added to PlantSeg version 1.1.8. These features can be used only after installing the SimpleITK package:</p> <pre><code>conda activate plant-seg\npip install SimpleITK\n</code></pre>"},{"location":"chapters/getting_started/quick_start/","title":"Quick Start","text":""},{"location":"chapters/getting_started/quick_start/#pipeline-usage-napari-viewer","title":"Pipeline Usage (Napari viewer)","text":"<p>PlantSeg app can also be started using napari as a viewer. First, activate the newly created conda environment with: <pre><code>conda activate plant-seg\n</code></pre></p> <p>then, start the plantseg in napari <pre><code>$ plantseg --napari\n</code></pre> A more in depth guide can be found in our documentation (GUI).</p>"},{"location":"chapters/getting_started/quick_start/#pipeline-usage-gui","title":"Pipeline Usage (GUI)","text":"<p>PlantSeg app can also be started in a GUI mode, where basic user interface allows to configure and run the pipeline. First, activate the newly created conda environment with: <pre><code>conda activate plant-seg\n</code></pre></p> <p>then, run the GUI by simply typing: <pre><code>$ plantseg --gui\n</code></pre> A more in depth guide can be found in our documentation (Classic GUI).</p>"},{"location":"chapters/getting_started/quick_start/#pipeline-usage-command-line","title":"Pipeline Usage (command line)","text":"<p>Our pipeline is completely configuration file based and does not require any coding.</p> <p>First, activate the newly created conda environment with: <pre><code>conda activate plant-seg\n</code></pre> then, one can just start the pipeline with <pre><code>plantseg --config CONFIG_PATH\n</code></pre> where <code>CONFIG_PATH</code> is the path to the YAML configuration file. See config.yaml for a sample configuration file and our documentation (CLI) for a detailed description of the parameters.</p>"},{"location":"chapters/getting_started/quick_start/#using-liftedmulticut-segmentation","title":"Using LiftedMulticut segmentation","text":"<p>As reported in our paper, if one has a nuclei signal imaged together with the boundary signal, we could leverage the fact that one cell contains only one nucleus and use the <code>LiftedMultict</code> segmentation strategy and obtain improved segmentation. This workflow is now available in all PlantSeg interfaces.</p>"},{"location":"chapters/getting_started/troubleshooting/","title":"Troubleshooting","text":"<ul> <li><code>Could not load library libcudnn_ops_infer.so.8.</code></li> <li><code>NVIDIA driver on your system is too old</code> or <code>Torch not compiled with CUDA enabled</code></li> <li><code>RuntimeError: key : KEY_NAME is missing, plant-seg requires KEY_NAME to run</code></li> <li><code>cannot import name 'lifted_problem_from_probabilities'</code></li> <li>Other issues</li> </ul>"},{"location":"chapters/getting_started/troubleshooting/#could-not-load-library-libcudnn_ops_inferso8","title":"<code>Could not load library libcudnn_ops_infer.so.8.</code>","text":"<p>If you stumble in the following error message: <pre><code>Could not load library libcudnn_ops_infer.so.8. Error: libcudnn_ops_infer.so.8: cannot open shared object file: No such file or directory\n</code></pre></p> <p>Just install <code>cudnn</code> by running: <pre><code>$ mamba install -c conda-forge cudnn\n</code></pre></p>"},{"location":"chapters/getting_started/troubleshooting/#nvidia-driver-on-your-system-is-too-old-or-torch-not-compiled-with-cuda-enabled","title":"<code>NVIDIA driver on your system is too old</code> or <code>Torch not compiled with CUDA enabled</code>","text":"<p>If you stumble in the following error message: <pre><code>AssertionError:\nThe NVIDIA driver on your system is too old (found version xxxx).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: http://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver.\n</code></pre> or: <pre><code>    raise AssertionError(\"Torch not compiled with CUDA enabled\")\nAssertionError: Torch not compiled with CUDA enabled\n</code></pre> It means that your cuda installation does not match the default in plantseg. You can check your current cuda version by typing in the terminal <pre><code>cat /usr/local/cuda/version.txt\n</code></pre> Then you can re-install the pytorch version compatible with your cuda by activating your <code>plant-seg</code> environment: <pre><code>conda activate plant-seg\n</code></pre> and <pre><code>conda install -c pytorch torchvision cudatoolkit=&lt;YOU_CUDA_VERSION&gt; pytorch\n</code></pre> e.g. for cuda 9.2 <pre><code>conda install -c pytorch torchvision cudatoolkit=9.2 pytorch\n</code></pre></p> <p>Alternatively one can create the <code>plant-seg</code> environment from scratch and ensuring the correct version of cuda/pytorch, by: <pre><code>conda create -n plant-seg -c lcerrone -c abailoni -c cpape -c conda-forge cudatoolkit=&lt;YOU_CUDA_VERSION&gt; plantseg\n</code></pre></p>"},{"location":"chapters/getting_started/troubleshooting/#runtimeerror-key-key_name-is-missing-plant-seg-requires-key_name-to-run","title":"<code>RuntimeError: key : KEY_NAME is missing, plant-seg requires KEY_NAME to run</code>","text":"<p>If you use plantseg from the GUI and you receive an error similar to: <pre><code>RuntimeError: key : 'crop_volume' is missing, plant-seg requires 'crop_volume' to run\n</code></pre> (or a similar message for any of the other keys) It might be that the last session configuration file got corrupted or is outdated. You should be able to solve it by removing the corrupted file <code>config_gui_last.yaml</code>.</p> <p>If you have a standard installation of plantseg, you can remove it by executing on the terminal: <pre><code>$ rm ~/.plantseg_models/configs/config_gui_last.yaml\n</code></pre></p> <p>If you use plantseg from the command line, and you receive an error similar to: <pre><code>RuntimeError: key : 'crop_volume' is missing, plant-seg requires 'crop_volume' to run\n</code></pre></p> <p>Please make sure that your configuration has the correct formatting and contains all required keys. An updated example can be found inside the directory <code>examples</code>, in this repository.</p>"},{"location":"chapters/getting_started/troubleshooting/#cannot-import-name-lifted_problem_from_probabilities","title":"<code>cannot import name 'lifted_problem_from_probabilities'</code>","text":"<p>If when trying to execute the Lifted Multicut pipeline you receive an error like: <pre><code>'cannot import name 'lifted_problem_from_probabilities' from 'elf.segmentation.features''\n</code></pre> The solution is to re-install elf via <pre><code>conda install -c conda-forge python-elf\n</code></pre></p>"},{"location":"chapters/getting_started/troubleshooting/#other-issues","title":"Other issues","text":"<ul> <li>PlantSeg is under active development, so it may happen that the models/configuration files saved in <code>~/.plantseg_modes</code> are outdated. In case of errors related to loading the configuration file, please close the PlantSeg app, remove <code>~/.plantseg_models</code> directory and try again.</li> </ul>"},{"location":"chapters/plantseg_classic_cli/","title":"PlantSeg Classic CLI","text":""},{"location":"chapters/plantseg_classic_cli/#guide-to-custom-configuration-file","title":"Guide to Custom Configuration File","text":"<p>The configuration file defines all the operations in our pipeline together with the data to be processed. Please refer to config.yaml for a sample pipeline configuration and a detailed explanation of all parameters.</p>"},{"location":"chapters/plantseg_classic_cli/#main-keyssteps","title":"Main Keys/Steps","text":"<ul> <li><code>path</code> attribute: is used to define either the file to process or the directory containing the data.</li> <li><code>preprocessing</code> attribute: contains a simple set of possible operations one would need to run on their data before calling the neural network. This step can be skipped if data is ready for neural network processing. Detailed instructions can be found at Classic GUI (Data Processing).</li> <li><code>cnn_prediction</code> attribute: contains all parameters relevant for predicting with a neural network. Description of all pre-trained models provided with the package is described below. Detailed instructions can be found at Classic GUI (Predictions).</li> <li><code>segmentation</code> attribute: contains all parameters needed to run the partitioning algorithm (i.e., final Segmentation). Detailed instructions can be found at Classic GUI (Segmentation).</li> </ul>"},{"location":"chapters/plantseg_classic_cli/#additional-information","title":"Additional information","text":"<p>The PlantSeg-related files (models, configs) will be placed inside your home directory under <code>~/.plantseg_models</code>.</p> <p>Our pipeline uses the PyTorch library for CNN predictions. PlantSeg can be run on systems without GPU, however for maximum performance, we recommend that the application is run on a machine with a high-performance GPU for deep learning. If the <code>CUDA_VISIBLE_DEVICES</code> environment variable is not specified, the prediction task will be distributed on all available GPUs. E.g. run: <code>CUDA_VISIBLE_DEVICES=0 plantseg --config CONFIG_PATH</code> to restrict prediction to a given GPU.</p>"},{"location":"chapters/plantseg_classic_cli/#configuration-file-example","title":"configuration file example","text":"<p>This modality of using PlantSeg is particularly suited for high throughput processing and for running PlantSeg on a remote server. To use PlantSeg from command line mode, you will need to create a configuration file using a standard text editor  or using the save option of the PlantSeg GUI.</p> <p>Here is an example configuration:</p> <pre><code>path: /home/USERNAME/DATA.tiff # Contains the path to the directory or file to process\n\npreprocessing:\n  # enable/disable preprocessing\n  state: True\n  # create a new sub folder where all results will be stored\n  save_directory: \"PreProcessing\"\n  # rescaling the volume is essential for the generalization of the networks. The rescaling factor can be computed as the resolution\n  # of the volume at hand divided by the resolution of the dataset used in training. Be careful, if the difference is too large check for a different model.\n  factor: [1.0, 1.0, 1.0]\n  # the order of the spline interpolation\n  order: 2\n  # optional: perform Gaussian smoothing or median filtering on the input.\n  filter:\n    # enable/disable filtering\n    state: False\n    # Accepted values: 'gaussian'/'median'\n    type: gaussian\n    # sigma (gaussian) or disc radius (median)\n    param: 1.0\n\ncnn_prediction:\n  # enable/disable UNet prediction\n  state: True\n  # Trained model name, more info on available models and custom models in the README\n  model_name: \"generic_confocal_3D_unet\"\n  # If a CUDA capable gpu is available and corrected setup use \"cuda\", if not you can use \"cpu\" for cpu only inference (slower)\n  device: \"cpu\"\n  # how many subprocesses to use for data loading\n  num_workers: 8\n  # patch size given to the network (adapt to fit in your GPU mem)\n  patch: [32, 128, 128]\n  # stride between patches will be computed as `stride_ratio * patch`\n  # recommended values are in range `[0.5, 0.75]` to make sure the patches have enough overlap to get smooth prediction maps\n  stride_ratio: 0.75\n  # If \"True\" forces downloading networks from the online repos\n  model_update: False\n\ncnn_postprocessing:\n  # enable/disable cnn post processing\n  state: False\n  # if True convert to result to tiff\n  tiff: False\n  # rescaling factor\n  factor: [1, 1, 1]\n  # spline order for rescaling\n  order: 2\n\nsegmentation:\n  # enable/disable segmentation\n  state: True\n  # Name of the algorithm to use for inferences. Options: MultiCut, MutexWS, GASP, DtWatershed\n  name: \"MultiCut\"\n  # Segmentation specific parameters here\n  # balance under-/over-segmentation; 0 - aim for undersegmentation, 1 - aim for oversegmentation. (Not active for DtWatershed)\n  beta: 0.5\n  # directory where to save the results\n  save_directory: \"MultiCut\"\n  # enable/disable watershed\n  run_ws: True\n  # use 2D instead of 3D watershed\n  ws_2D: True\n  # probability maps threshold\n  ws_threshold: 0.5\n  # set the minimum superpixels size\n  ws_minsize: 50\n  # sigma for the gaussian smoothing of the distance transform\n  ws_sigma: 2.0\n  # sigma for the gaussian smoothing of boundary\n  ws_w_sigma: 0\n  # set the minimum segment size in the final segmentation. (Not active for DtWatershed)\n  post_minsize: 50\n\nsegmentation_postprocessing:\n  # enable/disable segmentation post processing\n  state: False\n  # if True convert to result to tiff\n  tiff: False\n  # rescaling factor\n  factor: [1, 1, 1]\n  # spline order for rescaling (keep 0 for segmentation post processing\n  order: 0\n</code></pre> <p>This configuration can be found at config.yaml.</p>"},{"location":"chapters/plantseg_classic_cli/#pipeline-usage-command-line","title":"Pipeline Usage (command line)","text":"<p>To start PlantSeg from the command line. First, activate the newly created conda environment with:</p> <pre><code>conda activate plant-seg\n</code></pre> <p>then, one can just start the pipeline with</p> <pre><code>plantseg --config CONFIG_PATH\n</code></pre> <p>where <code>CONFIG_PATH</code> is the path to a YAML configuration file.</p>"},{"location":"chapters/plantseg_classic_cli/#data-parallelism","title":"Data Parallelism","text":"<p>In the headless mode (i.e. when invoked with <code>plantseg --config CONFIG_PATH</code>) the prediction step will run on all the GPUs using DataParallel. If prediction on all available GPUs is not desirable, restrict the number of GPUs using <code>CUDA_VISIBLE_DEVICES</code>, e.g.</p> <pre><code>CUDA_VISIBLE_DEVICES=0,1 plantseg --config CONFIG_PATH\n</code></pre>"},{"location":"chapters/plantseg_classic_cli/#results","title":"Results","text":"<p>The results are stored together with the source input files inside a nested directory structure. As an example, if we want to run PlantSeg inside a directory with two stacks, we will obtain the following outputs:</p> <pre><code>/file1.tif\n/file2.tif\n/PreProcesing/\n------------&gt;/file1.h5\n------------&gt;/file1.yaml\n------------&gt;/file2.h5\n------------&gt;/file2.yaml\n------------&gt;/generic_confocal_3d_unet/\n-------------------------------------&gt;/file1_predictions.h5\n-------------------------------------&gt;/file1_predictions.yaml\n-------------------------------------&gt;/file2_predictions.h5\n-------------------------------------&gt;/file2_predictions.yaml\n-------------------------------------&gt;/GASP/\n------------------------------------------&gt;/file_1_predions_gasp_average.h5\n------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n------------------------------------------&gt;/file_2_predions_gasp_average.h5\n------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n------------------------------------------&gt;/PostProcessing/\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n</code></pre> <p>The use of this hierarchical directory structure allows PlantSeg to find the necessary files quickly and can be used to test different segmentation algorithms/parameter combinations minimizing the memory overhead on the disk. For the sake of reproducibility, every file is associated with a configuration file \".yaml\" that saves all parameters used to produce the result.</p>"},{"location":"chapters/plantseg_classic_cli/#liftedmulticut-segmentation","title":"LiftedMulticut segmentation","text":"<p>As reported in our paper, if one has a nuclei signal imaged together with the boundary signal, we could leverage the fact that one cell contains only one nucleus and use the <code>LiftedMultict</code> segmentation strategy and obtain improved segmentation. We will use the Arabidopsis thaliana lateral root as an example. The <code>LiftedMulticut</code> strategy consists of running PlantSeg two times:</p> <ol> <li> <p>Using PlantSeg to predict the nuclei probability maps using the <code>lightsheet_unet_bce_dice_nuclei_ds1x</code> network. In this case, only the pre-processing and CNN prediction steps are enabled in the config. See example config.</p> <pre><code>plantseg --config nuclei_predictions_example.yaml\n</code></pre> </li> <li> <p>Using PlantSeg to segment the input image with the <code>LiftedMulticut</code> algorithm given the nuclei probability maps from the 1st step. See example config. The notable difference is that in the <code>segmentation</code> part of the config, we set <code>name: LiftedMulticut</code> and the <code>nuclei_predictions_path</code> as the path to the directory where the nuclei pmaps were saved in step 1. Also, make sure that the <code>path</code> attribute points to the raw files containing the cell boundary staining (NOT THE NUCLEI).</p> <pre><code>plantseg --config lifted_multicut_example.yaml\n</code></pre> </li> </ol> <p>If case when the nuclei segmentation is given, one should skip step 1., add <code>is_segmentation=True</code> flag in the config and directly run step 2.</p>"},{"location":"chapters/plantseg_classic_gui/","title":"PlantSeg from GUI","text":"<p>The graphical user interface is the easiest way to configure and run PlantSeg. Currently the GUI does not allow to visualize or interact with the data. We recommend using MorphographX or Fiji in order to assert the success and quality of the pipeline results.</p> <p></p>"},{"location":"chapters/plantseg_classic_gui/#file-browser-widget","title":"File Browser Widget","text":"<p>The file browser can be used to select the input files for the pipeline. PlantSeg can run on a single file (button A) or in batch mode for all files inside a directory (button B). If a directory is selected PlantSeg will run on all compatible files inside the directory.</p>"},{"location":"chapters/plantseg_classic_gui/#main-pipeline-configurator","title":"Main Pipeline Configurator","text":"<p>The central panel of PlantSeg (C) is the core of the pipeline configuration. It can be used for customizing and tuning the pipeline accordingly to the data at hand. Detailed information for each stage can be found at: * Data-Processing * CNN-Predictions * Segmentation</p> <p>Any of the above widgets can be run singularly or in sequence (left to right). The order of execution can not be modified.</p>"},{"location":"chapters/plantseg_classic_gui/#run","title":"Run","text":"<p>The last panel has two main functions. Running the pipeline (D), once the run button is pressed the pipeline starts. The button is inactive until the process is finished. Adding a custom model (E). Custom trained model can be done by using the dedicated popup. Training a new model can be done following the instruction at pytorch-3dunet.</p>"},{"location":"chapters/plantseg_classic_gui/#results","title":"Results","text":"<p>The results are stored together with the source input files inside a nested directory structure. As example, if we want to run PlantSeg inside a directory with 2 stacks, we will obtain the following outputs: <pre><code>/file1.tif\n/file2.tif\n/PreProcesing/\n------------&gt;/file1.h5\n------------&gt;/file1.yaml\n------------&gt;/file2.h5\n------------&gt;/file2.yaml\n------------&gt;/generic_confocal_3d_unet/\n-------------------------------------&gt;/file1_predictions.h5\n-------------------------------------&gt;/file1_predictions.yaml\n-------------------------------------&gt;/file2_predictions.h5\n-------------------------------------&gt;/file2_predictions.yaml\n-------------------------------------&gt;/GASP/\n------------------------------------------&gt;/file_1_predions_gasp_average.h5\n------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n------------------------------------------&gt;/file_2_predions_gasp_average.h5\n------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n------------------------------------------&gt;/PostProcessing/\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n</code></pre> The use of this hierarchical directory structure allows PlantSeg to easily find the necessary files and can be used to test different combination of segmentation algorithms/parameters minimizing the memory overhead on the disk. For sake of reproducibility, every file is associated with a configuration file \".yaml\" that saves all parameters used to produce the result.</p>"},{"location":"chapters/plantseg_classic_gui/#start-plantseg-gui","title":"Start PlantSeg GUI","text":"<p>In order to start the PlantSeg app in GUI mode: First, activate the newly created conda environment with: <pre><code>conda activate plant-seg\n</code></pre></p> <p>then, run the GUI by simply typing: <pre><code>$ plantseg --gui\n</code></pre></p>"},{"location":"chapters/plantseg_classic_gui/cnn_predictions/","title":"CNN Predictions","text":"<p>The CNN predictions widget process the stacks at hand with a Convolutional Neural Network. The output is a boundary classification image, where every voxel gets a value between 0 (not a cell boundary) and 1 (cell boundary).</p> <p>The input image can be a raw stack \"tiff\"/\"h5\" or the output of the PreProcessing widget.</p> <ul> <li> <p>The Model Name menu shows all available models. There are two main basic models available</p> <ol> <li> <p>Generic confocalis a generic model for all confocal datasets. Some examples: </p> </li> <li> <p>Generic lightsheet this is a generic model for all lightsheet datasets.  Some examples:  </p> </li> </ol> </li> <li> <p>Due to memory constraints, usually a complete stack does not fit the GPUs memory,  therefore the Patch size can be used to optimize the performance of the pipeline.  Usually, larger patches cost more memory but can slightly improve performance.  For 2D segmentation, the Patch size relative to the z-axis has to be set to 1.</p> </li> <li> <p>To minimize the boundary effect due to the sliding windows patching, we can use different stride:</p> <ol> <li>Accurate: corresponding to a stride 50% of the patch size (yield best predictions/segmentation accuracy)</li> <li>Balanced: corresponding to a stride 75% of the patch size</li> <li>Draft: corresponding to a stride 95% of the patch size (yield fastest runtime)</li> </ol> </li> <li> <p>The Device type menu can be used to enable or not GPU acceleration. CUDA greatly accelerates the network predictions on Nvidia GPUs. At the moment, we don't support other GPUs manufacturers.</p> </li> </ul>"},{"location":"chapters/plantseg_classic_gui/data_processing/","title":"Classic Data Processing","text":"<p> PlantSeg includes essential utilities for data pre-processing and post-processing.</p>"},{"location":"chapters/plantseg_classic_gui/data_processing/#pre-processing","title":"Pre-Processing","text":"<p>The input for this widget can be either a \"raw\" image or a \"prediction\" image. Input formats allowed are tiff and h5, while output is always h5.</p> <ul> <li> <p>Save Directory can be used to define the output directory.</p> </li> <li> <p>The most critical setting is the Rescaling. It is important to rescale the image to  match the resolution of the data used for training the Neural Network. This operation can be done automatically by clicking on the GUI on Guided. Be careful to use this function only in case of data considerably different from the reference resolution. <pre><code>As an example:\n  - if your data has the voxel size of 0.3 x 0.1 x 0.1 (ZYX).\n  - and the networks was trained on 0.3 x 0.2 x 0.2 data (reference resolution).\n\nThe required voxel size can be obtained by computing the ratio between your data and the\nreference train dataset. In the example the rescaling factor = 1 x 2 x 2.\n</code></pre></p> </li> <li> <p>The Interpolation field controls the interpolation type (0 for nearest neighbors, 1 for linear spline, 2 for quadratic).</p> </li> <li> <p>The last field defines a Filter operation. Implemented there are:</p> <ol> <li>Gaussian Filtering: The parameter is a float and defines the sigma value for the gaussian smoothing. The higher, the wider is filtering kernel.</li> <li>Median Filtering: Apply median operation convolutionally over the image.  The kernel is a sphere of size defined in the parameter field.</li> </ol> </li> </ul>"},{"location":"chapters/plantseg_classic_gui/data_processing/#post-processing","title":"Post-Processing","text":"<p>A post-processing step can be performed after the CNN-Predictions and the Segmentation. The post-processing options are:  * Converting the output to the tiff file format (default is h5).</p> <ul> <li>Casting the CNN-Predictions output to data_uint8 drastically reduces the memory footprint of the output  file.</li> </ul> <p>Additionally, the post-processing will scale back your outputs to the original voxels resolutions.</p>"},{"location":"chapters/plantseg_classic_gui/segmentation/","title":"Segmentation","text":"<p>The segmentation widget allows using very powerful graph partitioning techniques to obtain a segmentation from the input stacks. The input of this widget should be the output of the CNN-predictions widget. If the boundary prediction stage fails for any reason, a raw image could be used (especially if the cell boundaries are  very sharp, and the noise is low) but usually does not yield satisfactory results.</p> <ul> <li> <p>The Algorithm menu can be used to choose the segmentation algorithm. Available choices are:</p> <ol> <li>GASP (average): is a generalization of the classical hierarchical clustering. It usually delivers very reliable and accurate segmentation. It is the default in PlantSeg.</li> <li>MutexWS: Mutex Watershed is a derivative of the standard Watershed, where we do not need seeds for the  segmentation. This algorithm performs very well in certain types of complex morphology (like )</li> <li>MultiCut: in contrast to the other algorithms is not based on a greedy agglomeration but tries to find the optimal global segmentation. This is, in practice, very hard, and it can be infeasible for huge stacks.</li> <li>DtWatershed: is our implementation of the distance transform Watershed. From the input, we extract a distance map from the boundaries. Based this distance map, seeds are placed at local minima. Then those seeds are used for computing the Watershed segmentation. To speed up the computation of GASP, MutexWS, and MultiCut, an over-segmentation  is obtained using Dt Watershed.</li> </ol> </li> <li> <p>Save Directory defines the sub-directory's name where the segmentation results will be stored.</p> </li> <li> <p>The Under/Over- segmentation factor is the most critical parameters for tuning the segmentation of GASP, MutexWS and MultiCut. A small value will steer the segmentation towards under-segmentation. While a high-value bias the segmentation towards the over-segmentation. This parameter does not affect the distance transform Watershed.</p> </li> <li> <p>If Run Watershed in 2D value is True, the superpixels are created in 2D (over the z slice). While if False makes the superpixels in the whole 3D volume. 3D superpixels are much slower and memory intensive but can improve  the segmentation accuracy.</p> </li> <li> <p>The CNN Predictions Threshold is used for the superpixels extraction and Distance Transform Watershed. It has a crucial role for the watershed seeds extraction and can be used similarly to the \"Unde/Over segmentation factor.\" to bias the final result. A high value translates to less seeds being placed (more under segmentation), while with a low value, more seeds are  placed (more over-segmentation).</p> </li> <li> <p>The input is used by the distance transform Watershed to extract the seed and find the segmentation boundaries. If Watershed Seeds Sigma and Watershed Boundary Sigma are larger than  zero, a gaussian smoothing is applied on the input before the operations above. This is mainly helpful for  the seeds computation but, in most cases, does not impact segmentation quality.</p> </li> <li> <p>The Superpixels Minimum Size applies a size filter to the initial superpixels over-segmentation. This removes Watershed often produces small segments and is usually helpful for the subsequent agglomeration.  Segments smaller than the threshold will be merged with the nearest neighbor segment.</p> </li> <li> <p>Even though GASP, MutexWS, and MultiCut are not very prone to produce small segments, the Cell Minimum Size can be used as a final size processing filter. Segments smaller than the threshold will be merged with the nearest neighbor cell.</p> </li> </ul>"},{"location":"chapters/plantseg_interactive_napari/","title":"PlantSeg Interactive - Napari","text":"<p>Documentation in Progress</p> <p>This page is under development.</p> <p>PlantSeg app can also be started using napari as a viewer. First, activate the newly created conda environment with:</p> <pre><code>conda activate plant-seg\n</code></pre> <p>then, start the plantseg in napari</p> <pre><code>plantseg --napari\n</code></pre>"},{"location":"chapters/plantseg_interactive_napari/data_processing/","title":"Data Processing","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/plantseg_interactive_napari/extra_pred/","title":"Extra Pred","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/plantseg_interactive_napari/extra_seg/","title":"Extra Seg","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/plantseg_interactive_napari/headless_batch_processing/","title":"Headless Batch Processing","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/plantseg_interactive_napari/unet_gasp_workflow/","title":"UNet GASP Workflow","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/plantseg_interactive_napari/unet_training/","title":"UNet Training","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/python_api/","title":"PlantSeg Python API","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/python_api/cnn_predictions/","title":"PlantSeg CNN Predictions","text":"<p>In this section we will describe how to use the PlantSeg CNN Predictions workflow from the python API.</p>"},{"location":"chapters/python_api/cnn_predictions/#api-reference-plantsegpredictionsfunctionalpredictions","title":"API-Reference: plantseg.predictions.functional.predictions","text":""},{"location":"chapters/python_api/cnn_predictions/#unet_predictions","title":"unet_predictions","text":"<p><pre><code>def unet_predictions(raw: np.array,\n                     model_name: str,\n                     patch: Tuple[int, int, int] = (80, 160, 160),\n                     stride: Union[str, Tuple[int, int, int]] = 'Accurate (slowest)',\n                     device: str = 'cuda',\n                     version: str = 'best',\n                     model_update: bool = False,\n                     mirror_padding: Tuple[int, int, int] = (16, 32, 32),\n                     disable_tqdm: bool = False) -&gt; np.array:\n    \"\"\"\n    Predict boundaries predictions from raw data using a 3D U-Net model.\n\n    Args:\n        raw (np.array): raw data, must be a 3D array of shape (Z, Y, X) normalized between 0 and 1.\n        model_name (str): name of the model to use. A complete list of available models can be found here:\n        patch (tuple[int, int, int], optional): patch size to use for prediction. Defaults to (80, 160, 160).\n        stride (Union[str, tuple[int, int, int]], optional): stride to use for prediction.\n            If stride is defined as a string must be one of ['Accurate (slowest)', 'Balanced', 'Draft (fastest)'].\n            Defaults to 'Accurate (slowest)'.\n        device: (str, optional): device to use for prediction. Must be one of ['cpu', 'cuda', 'cuda:1', etc.].\n            Defaults to 'cuda'.\n        version (str, optional): version of the model to use, must be either 'best' or 'last'. Defaults to 'best'.\n        model_update (bool, optional): if True will update the model to the latest version. Defaults to False.\n        mirror_padding (tuple[int, int, int], optional): padding to use for prediction. Defaults to (16, 32, 32).\n        disable_tqdm (bool, optional): if True will disable tqdm progress bar. Defaults to False.\n\n    Returns:\n        np.array: predictions, 3D array of shape (Z, Y, X) with values between 0 and 1.\n\n    \"\"\"\n\n    ...\n</code></pre> <pre><code># Minimal example\nfrom plantseg.predictions.functional.predictions import unet_predictions\nimport numpy as np\n\nraw = np.random.random((128, 256, 256))\npredictions = unet_predictions(raw,\n                               model_name='generic_confocal_3d_unet',\n                               patch=(80, 160, 160),\n                               device='cuda')\n</code></pre></p>"},{"location":"chapters/python_api/data_processing/","title":"Data Processing","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/python_api/segmentation/","title":"PlantSeg Segmentation","text":"<p>In this section we will describe how to use the PlantSeg segmentation workflows from the python API.</p>"},{"location":"chapters/python_api/segmentation/#api-reference-plantsegpredictionsfunctionalsegmentation","title":"API-Reference: plantseg.predictions.functional.segmentation","text":"<ul> <li>dt_watershed <pre><code>def dt_watershed(boundary_pmaps: np.array,\n                 threshold: float = 0.5,\n                 sigma_seeds: float = 1.,\n                 stacked: bool = False,\n                 sigma_weights: float = 2.,\n                 min_size: int = 100,\n                 alpha: float = 1.0,\n                 pixel_pitch: tuple[int, ...] = None,\n                 apply_nonmax_suppression: bool = False,\n                 n_threads: int = None,\n                 mask: np.array = None) -&gt; np.array:\n    \"\"\" Wrapper around elf.distance_transform_watershed\n    Args:\n        boundary_pmaps (np.ndarray): input height map.\n        threshold (float): value for the threshold applied before distance transform.\n        sigma_seeds (float): smoothing factor for the watershed seed map.\n        stacked (bool): if true the ws will be executed in 2D slice by slice, otherwise in 3D.\n        sigma_weights (float): smoothing factor for the watershed weight map (default: 2).\n        min_size (int): minimal size of watershed segments (default: 100)\n        alpha (float): alpha used to blend input_ and distance_transform in order to obtain the\n            watershed weight map (default: .9)\n        pixel_pitch (list-like[int]): anisotropy factor used to compute the distance transform (default: None)\n        apply_nonmax_suppression (bool): whether to apply non-maximum suppression to filter out seeds.\n            Needs nifty. (default: False)\n        n_threads (int): if not None, parallelize the 2D stacked ws. (default: None)\n        mask (np.ndarray)\n    Returns:\n        np.ndarray: watershed segmentation\n    \"\"\"\n\n    ...\n</code></pre></li> </ul> <p>https://github.com/hci-unihd/plant-seg/blob/de397694f523f142d67a38d5611acefd03e33137/plantseg/segmentation/functional/segmentation.py#L23-L122 * gasp <pre><code>def gasp(boundary_pmaps: np.array,\n         superpixels: np.array = None,\n         gasp_linkage_criteria: str = 'average',\n         beta: float = 0.5,\n         post_minsize: int = 100,\n         n_threads: int = 6) -&gt; np.array:\n    \"\"\"\n    Implementation of the GASP algorithm for segmentation from affinities.\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary predictions.\n        superpixels (np.ndarray): superpixel segmentation. If None, GASP will be run from the pixels. (default: None)\n        gasp_linkage_criteria (str): Linkage criteria for GASP. (default: 'average')\n        beta (float): beta parameter for GASP. A small value will steer the segmentation towards under-segmentation.\n        While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after GASP. (default: 100)\n        n_threads (int): number of threads used for GASP. (default: 6)\n    Returns:\n        np.ndarray: GASP output segmentation\n    \"\"\"\n\n    ...\n</code></pre></p> <ul> <li> <p>mutex_ws <pre><code>def mutex_ws(boundary_pmaps: np.array,\n             superpixels: np.array = None,\n             beta: float = 0.5,\n             post_minsize: int = 100,\n             n_threads: int = 6) -&gt; np.array:\n    \"\"\"\n    Wrapper around gasp with mutex_watershed as linkage criteria.\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary predictions. 3D array of shape (Z, Y, X) with values between 0 and 1.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n            If None, GASP will be run from the pixels. (default: None)\n        beta (float): beta parameter for GASP. A small value will steer the segmentation towards under-segmentation.\n            While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after GASP. (default: 100)\n        n_threads (int): number of threads used for GASP. (default: 6)\n    Returns:\n        np.ndarray: GASP output segmentation\n    \"\"\"\n\n    ...\n</code></pre></p> </li> <li> <p>multicut <pre><code>def multicut(boundary_pmaps: np.array,\n             superpixels: np.array,\n             beta: float = 0.5,\n             post_minsize: int = 50) -&gt; np.array:\n\n    \"\"\"\n    Multicut segmentation from boundary predictions.\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary predictions, 3D array of shape (Z, Y, X) with values between 0 and 1.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n        beta (float): beta parameter for the Multicut. A small value will steer the segmentation towards\n            under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after Multicut. (default: 100)\n    Returns:\n        np.ndarray: Multicut output segmentation\n    \"\"\"\n\n    ...\n</code></pre></p> </li> <li> <p>lifted_multicut_from_nuclei_segmentation <pre><code>def lifted_multicut_from_nuclei_segmentation(boundary_pmaps: np.array,\n                                             nuclei_seg: np.array,\n                                             superpixels: np.array,\n                                             beta: float = 0.5,\n                                             post_minsize: int = 50) -&gt; np.array:\n    \"\"\"\n    Lifted Multicut segmentation from boundary predictions and nuclei segmentation.\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary predictions, 3D array of shape (Z, Y, X) with values between 0 and 1.\n        nuclei_seg (np.array): Nuclei segmentation. Must have the same shape as boundary_pmaps.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n        beta (float): beta parameter for the Multicut. A small value will steer the segmentation towards\n        under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after Multicut. (default: 100)\n    Returns:\n        np.ndarray: Multicut output segmentation\n    \"\"\"\n\n    ...\n</code></pre></p> </li> </ul>"}]}